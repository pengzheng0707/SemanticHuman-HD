<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="SemanticHuman-HD: High-Resolution Semantic Disentangled 3D Human Generation">
  <meta name="keywords" content="SemanticHuman-HD">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SemanticHuman-HD: High-Resolution Semantic Disentangled 3D Human Generation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">SemanticHuman-HD:</h1>
          <h3 class="title is-1 long-title">High-Resolution Semantic Disentangled 3D Human Generation</h3>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://pengzheng0707.github.io">Peng Zheng</a><sup>1</sup>,</span>
            <span class="author-block">
              <a>Tao Liu</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://zili-yi.github.io/">Zili Yi</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://ruim-jlu.github.io/">Rui Ma</a><sup>1,2</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>School of Artificial Intelligence, Jilin University, Changchun, China</span>
            <span class="author-block"><sup>2</sup>Engineering Research Center of Knowledge-Driven Human-Machine Intelligence, MOE, China</span>
            <span class="author-block"><sup>3</sup>School of Intelligence Science and Technology, Nanjing University, Suzhou, China</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2403.10166.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2403.10166"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a 
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/inv_00.png"
      class="teasor"
      alt="Semantic-aware virtual try-on image"/>
      <p class="content has-text-justified">
        (a) Semantic-aware virtual try-on. Given a real image, we first employ GAN inversion to obtain its semantic latent code. Subsequently, we replace the top and
        bottom garment by manipulating the semantic latent code. Here, the top is randomly
        generated by our model, and the bottom is disentangled from another GAN inversion
        result. 
        (b) Controllable image synthesis. Our method allows for generating the same person in different poses as well as rendering them from different viewpoints.
      </p>
    </div>
  </div>
</section>

<div class="container is-max-desktop">

  <div class="columns is-centered">

    <!-- Visual Effects. -->
    <div class="column">
      <h2 class="title is-3">Pose Control</h2>
      <div class="content">
        <video poster="" id="steve" autoplay controls muted loop playsinline height="300">
          <source src="./static/videos/anim1.mp4"
                  type="video/mp4">
        </video>
        <video poster="" id="steve" autoplay controls muted loop playsinline height="300">
          <source src="./static/videos/anim2.mp4"
                  type="video/mp4">
        </video>
        <video poster="" id="steve" autoplay controls muted loop playsinline height="300">
          <source src="./static/videos/anim3.mp4"
                  type="video/mp4">
        </video>
        <p class="content has-text-justified">
          With the pose control,
          viewers can see the generated 3D humans in motion,
          as well as the 3D garments.
        </p>
      </div>
    </div>
    <!--/ Visual Effects. -->

    <!-- Matting. -->
    <div class="column">
      <h2 class="title is-3">View Control</h2>
      <div class="columns is-centered">
        <div class="column content">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="300">
            <source src="./static/videos/3d1.mp4"
                    type="video/mp4">
          </video>
          <video poster="" id="steve" autoplay controls muted loop playsinline height="300">
            <source src="./static/videos/3d2.mp4"
                    type="video/mp4">
          </video>
          <video poster="" id="steve" autoplay controls muted loop playsinline height="300">
            <source src="./static/videos/3d3.mp4"
                    type="video/mp4">
          </video>
          <p class="content has-text-justified">
            We render generated results from different viewpoints.
    Notably, the rendering results include 3D garments.
          </p>
        </div>

      </div>
    </div>
  </div>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            With the development of neural radiance fields and generative models,
            numerous methods have been proposed for learning 3D human generation from 2D images.
            These methods allow control over the pose of the generated 3D human and enable rendering from different viewpoints. 
            However, none of these methods explore semantic disentanglement in human image synthesis, 
            i.e., they can not disentangle the generation of different semantic parts, 
            such as the body, tops, and bottoms. 
            Furthermore, existing methods are limited to synthesize images at 512<sup>2</sup>
            resolution due to the high computational cost of neural radiance fields. 
          </p>
          <p>
            To address these limitations, we introduce SemanticHuman-HD, 
            the first method to achieve semantic disentangled human image synthesis. 
            Notably, SemanticHuman-HD is also the first method to achieve 3D-aware image synthesis at 1024<sup>2</sup> resolution,
            benefiting from our proposed 3D-aware super-resolution module.
            By leveraging the depth maps and semantic masks as guidance for the 3D-aware super-resolution, 
            we significantly reduce the number of sampling points during volume rendering, 
            thereby reducing the computational cost. 
          </p>
          <p>
            Our comparative experiments demonstrate the superiority of our method. 
            The effectiveness of each proposed component is also verified through ablation studies. 
            Moreover, our method opens up exciting possibilities for various applications, 
            including 3D garment generation, semantic-aware image synthesis, controllable image synthesis, 
            and out-of-domain image synthesis. 
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <h2 class="title is-3">Results</h2>
    <h4 class="title is-4">Semantic disentanglement</h4>
    <section class="hero teaser">
      <div class="container is-max-desktop">
        <div class="hero-body">
          <img src="./static/images/change_00.png"
          class="teasor"
          alt="Semantic disentanglement image"/>
          <p class="content has-text-justified">
            By modifying the latent code of a
            specified semantic part, we can alter that specified part in the synthesized image.
          </p>
        </div>
      </div>
    </section>

    <h4 class="title is-4">Out-of-domain image synthesis</h4>
    <section class="hero teaser">
      <div class="container is-max-desktop">
        <div class="hero-body">
          <img src="./static/images/ood_00.png"
          class="teasor"
          alt="Out-of-domain image synthesis image"/>
          <p class="content has-text-justified">
            To achieve out-of-domain image synthesis, we
            assign different semantic labels to various semantic parts, e.g., if we set the semantic
            label corresponding to the body as "male", and the label corresponding to the tops as
            "dress", we can synthesize an image of a man wearing a dress.
          </p>
        </div>
      </div>
    </section>


    <h4 class="title is-4">Garment generation</h4>
    <section class="hero teaser">
      <div class="container is-max-desktop">
        <div class="hero-body">
          <img src="./static/images/garment_00.png"
          class="teasor"
          alt="Semantic-aware virtual try-on image"/>
          <p class="content has-text-justified">
            (a) Results randomly generated by our model. (b) Results obtained from GAN inversion.
          </p>
        </div>
      </div>
    </section>

    <h4 class="title is-4">Conditional image synthesis</h4>
    <section class="hero teaser">
      <div class="container is-max-desktop">
        <div class="hero-body">
          <img src="./static/images/cond_00.png"
          class="teasor"
          alt="Conditional image synthesis image"/>
          <p class="content has-text-justified">
            For each paired set of images, the image on the
right is synthesized conditioned on the semantic label <i>L<sub>s</sub></i> and human pose <i>P</i> of the real
image on the left.
          </p>
        </div>
      </div>
    </section>

    <h4 class="title is-4">Semantic-aware interpolation</h4>
    <section class="hero teaser">
      <div class="container is-max-desktop">
        <div class="hero-body">
          <img src="./static/images/interpolate_part_00.png"
          class="teasor"
          alt="Semantic-aware interpolation image"/>
          <p class="content has-text-justified">
            Red dashed rectangles on the images indicate
chosen semantic parts during the interpolation.
        </div>
      </div>
    </section>

    <h4 class="title is-4">3D garment interpolation</h4>
    <section class="hero teaser">
      <div class="container is-max-desktop">
        <div class="hero-body">
          <img src="./static/images//interpolate_garment_00.png"
          class="teasor"
          alt="3D garment interpolation image"/>
          <p class="content has-text-justified">
            3D garment interpolation, including images and normal maps. For a closer
            view, please zoom in to see the details.
        </div>
      </div>
    </section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This webpage is built with the template from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
